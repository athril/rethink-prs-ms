## Methods

### Multifactor Dimensionality Reduction (MDR) and model-based MDR (MB-MDR)
MDR is a nonparametric method that detects multiple genetic loci associated with a clinical outcome by reducing the dimension of a genotype dataset by pooling multilocus genotypes into high-risk and low-risk groups [@doi:10.1086/321276].
Extended from the original MDR algorithm, MB-MDR addresses existing limitations of MDR by increasing detectability of important interactions and decreasing bias by allowing O labels for individuals with no evidence for abberant risk.
Several improvements have been made to MB-MDR since it was first introduced in 2009, and its current implementation efficiently and effectively detects multiple sets of significant gene-gene interactions in relation to a trait of interest while efficiently controlling type I error rates.

In addition to the test statistic and P values associated with each genotype combination, another important output of MB-MDR is the HLO matrices generated from the affected- and unaffected-subjects matrices (in the case of binary outcome).
Briefly, for each genotype combination, an HLO matrix is a 3 x 3 matrix with each cell containing H (high), L (low) or O (no evidence), indicating risk of an individual whose genotype pairs fall into that cell [@doi:10.1186/1471-2105-14-138].
For an example binary outcome problem, a genotype combination $SNP_1$ and $SNP_2$ will have a $\chi^2$ value, an associated P value and an HLO matrix that looks like
$$ \begin{array}{l|ccc}
& SNP_1 = 0 & SNP_1 = 1 & SNP_1 = 2   \\
\hline
SNP_2 = 0 & O        & O        & O \\
SNP_2 = 1 & O        & H        & L \\
SNP_2 = 2 & O        & L        & H
\end{array}
$$
We discuss in the following subsection how these values were utilized in the formulation of the Multilocus Risk Score (MRS).

[More on the significance of SNP combination vs. significance of H/L/O here...]

### Multilocus Risk Score (MRS)
We apply the MB-MDR software [@doi:10.1186/1471-2105-14-138] v.4.4.1 to simulated datasets of $n$ individuals, $p$ SNPs to obtain the significance level of each combination of SNPs.
We let $k_d$ denote the number of significant combinations for a specific model dimension $d$ (e.g. $d = 2$ results in pairs of SNPs).
In this study, no significance threshold is imposed at the SNP combination level and, thus, $k_d$ reaches its maximum value of $C^d_p$ ($p$ choose $d$).

For each subject $i$ ($i = 1,2, \dotsm, n$), the $d$-way interaction risk score is calculated as
$$MRS_d(i) = \sum_{j = 1}^{k_d} \chi_j^2 \times \textrm{HLO}_j(X_{ij})$$
where $\chi_j^2$ is the test statistic of each genotype combination $j$ from a $\chi_j^2$ test with one degree of freedom for the simulated binary trait, $X_{ij}$ is the $j^{th}$ genotype combinations of subject $i$ and $\textrm{HLO}_j$ represents the $j^{th}$ recoded HLO matrix (1 = High, -1 = Low, 0 = No evidence).
In this study, we selected the default multiple testing correction algorithms for an MB-MDR model where the $\chi_j^2$ for each genotype combination is derived from the gammaMAXT algorithm for two tests: H versus LO and L versus HO.
As an example, consider a pair $X_{*j} = (SNP_{j_1}, SNP_{j_2})$ with $\chi_j^2=8.3$ and corresponding HLO matrix of all O's except an L in the first cell.
Then, all subjects' current risks would remain the same except the ones with $SNP_{j_1} = SNP_{j_2} = 0$ where their risks are subtracted by 8.3.

In this study, we consider 1-way and 2-way interactions and thus the combined risk is simply the total of the first two: $MRS = MRS_1 + MRS_2$.
We will examine the combined risk MRS and also its components, MRS1 and MRS2, separately.

### Mutual information and information gain
For a given simulated data set, we apply entropy-based methods to measure how much information about the phenotype is due to either marginal effects or the synergistic effects of the variants after subtracting the marginal effects.
A dataset's main effect (i.e. marginal effect $ME$) can be measured as the total of mutual information between each genotype $SNP_j$ and the phenotypic class $y$ based on Shannon's entropy $H$ [@doi:10.1002/j.1538-7305.1948.tb01338.x]:
$$ME = \sum_{j}^k I(SNP_j; y) = \sum_{j}^k H(y) - H(y|SNP_j).$$

We measure the 2-way interaction information (i.e. degree of synergistic effects of genotypes on the phenotype) of each dataset by summing the pairwise information gain between all pairs of genetic attributes.
Specifically, if we let $X_j$ denote the $j^{th}$ genotype combination $(SNP_{j_1}, SNP_{j_2})$, the total 2-way interaction gain (i.e. synergistic effects $SE$) is calculated as 
$$SE = \sum_{j}^kIG(X_j; y) = \sum_{j}^k I(SNP_{j_1}, SNP_{j_2}; y) - I(SNP_{j_1}; y) - I(SNP_{j_2}; y),$$
where $IG$ measures how much of the phenotypic class $y$ can be explained by the 2-way epistatic interaction within the genotype combination $X_j$.
We refer the reader to Ref. [@doi:10.1007/978-1-4939-2155-3_13] for more details on the calculation of the entropy-based terms.

### Simulated data
The major objective of data generation was providing a comprehensive set of reproducible and diverse enough datasets for the study. The data was generated in the following manner. The size of each dataset was arbitrary set to 1000 rows, which corresponded to samples in real dataset, and 10 columns (corresponding to SNPs). The values in the matrix were drawn using an uniform distribution among 4 potential options: with 1/2 probability of drawing '1' (representing heterozygous minor alleles aa), 1/4 probability of drawing '2' (representing a major homozygous allele AA) and 1/4 of drawing  '0' (representing homozygous major allele AA). The binary endpoint for the data was determined using a recently proposed evolutionary-based method for dataset generation called Heuristic Identification of Biological Architectures for simulating Complex Hierarchical Interactions (HIBACHI) [@doi:10.1142/9789813235533_0024].
This method uses Genetic Programming (GP) to build different mathematical and logical models resulting in a binary endpoint, such that the objective function called fitness is maximized. We set the fitness function of HIBACHI as a difference of the accuracies between two classifiers. The first classifier (called "up")  was supposed to perform as good as possible on the data, whilst the other (called "down") as bad as possible. At each iteration 5 randomly chosen settings of each machine learning methods were chosen among all potential options and served as hyperparameters for the method. Each data model was analyzed using 5-fold cross-validation and the best performing parameter setting was chosen. Each experiment in which two machine learning method was supposed to outperform the other was repeated 5 times. The machine learning methods that were picked for the study along with their parameters are presented in Table 1 

| Algorithm      |                             Parameters                                         |
|:--------------:|:------------------------------------------------------------------------------:|
| GaussianNB     |'priors': {None,2}, 'var_smoothing': {1e-9, 1e-7, 1e-5, 1e-3, 1e-1, 1e+1}       |
| BernoulliNB    |'alpha': {1e-3, 1e-2, 1e-1, 1., 10., 100.},'fit_prior': {True, False}           |
| DecisionTree   |'criterion': {"gini", "entropy"}, 'max_depth': [1, 11], 'min_samples_split': [2, 21], 'min_samples_leaf': [1, 21]|
|ExtraTrees      |'n_estimators': {50,100}, 'criterion': {"gini", "entropy"},'max_features': [0.1, 1], 'min_samples_split': range(2, 21,2),'min_samples_leaf': range(1, 21,2), 'bootstrap': {True, False}|
|RandomForest    |'n_estimators': {50,100}, 'criterion': {"gini", "entropy"},'max_features': [0.1, 1], 'min_samples_split': range(2, 21,4), 'min_samples_leaf':  range(1, 21,5), 'bootstrap': {True, False}                 |
|GradientBoosting|  'n_estimators': {50,100},'learning_rate': {1e-3, 1e-2, 1e-1, 0.5, 1}, 'max_depth': {2,4,8}, 'min_samples_split': [2, 21], 'min_samples_leaf': [1, 21], 'subsample': [0.1, 1], 'max_features': [0.1, 1]|
|KNeighbors      |'n_neighbors': [1, 100], 'weights': {"uniform", "distance"} , 'p': {1, 2}   |
|LinearSVC | 'penalty': {'l1','l2'}, 'C': {1e-4, 1e-3, 1e-2, 1, 1e2}, 'dual'=False |
|LogisticRegression| 'C': {1e-3, 1e-2, 1e-1, 1., 10.}, 'solver' : {'newton-cg', 'lbfgs', 'liblinear', 'sag'}|
|XGBoost     | 'n_estimators'=100, 'max_depth': [2, 10], 'learning_rate': {1e-3, 1e-2, 1e-1, 0.5, 1.}, 'subsample': [0.05, 1], 'min_child_weight': [1, 20],
|SVC | 'kernel'='rbf', 'C' : {1e-4, 1e-2, 1, 1e2}, 'gamma': {0.01, 0.1, 1, 10, 100}, | 
|MLPClassifier |: 'hidden_layer_sizes': {(10),(11),(12),(13),(14),(15),(10,5)},  'activation': {'logistic','tanh','relu'}, 'solver': ['lbfgs'], 'learning_rate': {'constant','invscaling'}, 'max_iter'=1000, 'alpha':np.logspace(-4,1,4)|


For each simulated and real-world dataset, after randomly splitting the entire data in two smaller sets (80% training and 20% holdout), we built the MRS model on training data to obtain the $\chi^2$ coefficients and the HLO matrix, and then we calculated risk score for each individual in the holdout set.
We assess the performance of the MRS by comparing the area under the Receiving Operator Characteristic curve (auROC) with that of the standard PRS method.

### Manuscript drafting
This manuscript is collaboratively written using the Manubot software which supports open paper writing via GitHub with Markdown [@doi:10.1371/journal.pcbi.1007128].
Manubot uses continuous integration to monitor changes and automatically update the manuscript.
As a result, the latest version of this manuscript is always available at [https://lelaboratoire.github.io/rethink-prs/](https://lelaboratoire.github.io/rethink-prs/).


### Availability
Detailed simulation and analysis code needed to reproduce the results in this study is available at [https://github.com/lelaboratoire/rethink-prs-ms/](https://github.com/lelaboratoire/rethink-prs-ms/).

